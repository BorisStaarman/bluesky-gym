episode_index,steps,mean_reward_drift,mean_reward_progress,mean_reward_intrusion,mean_reward_path_efficiency,mean_reward_boundary,mean_reward_step,mean_reward_proximity,sum_reward_drift,sum_reward_progress,sum_reward_intrusion,sum_reward_path_efficiency,sum_reward_boundary,sum_reward_step,sum_reward_proximity,total_intrusions,terminated_waypoint,truncated,finished_at
1,61,-0.002290165007999207,0.01307341691386788,0.0,0.0,-0.01639344262295082,-0.02000000000000001,0.0,-0.13970006548795164,0.7974784317459407,0.0,0.0,-1.0,-1.2200000000000006,0.0,0,False,True,1764576904.9919221
2,77,-0.0003794576020678329,0.04454515543238543,-0.9090909090909091,0.0,0.0,-0.02000000000000001,0.19600380069439,-0.029218235359223136,3.429976968293678,-70.0,0.0,0.0,-1.540000000000001,15.09229265346803,7,True,False,1764576920.1809704
