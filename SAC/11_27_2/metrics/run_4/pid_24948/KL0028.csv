episode_index,steps,mean_reward_drift,mean_reward_progress,mean_reward_intrusion,mean_reward_path_efficiency,mean_reward_boundary,mean_reward_step,mean_reward_proximity,sum_reward_drift,sum_reward_progress,sum_reward_intrusion,sum_reward_path_efficiency,sum_reward_boundary,sum_reward_step,sum_reward_proximity,total_intrusions,terminated_waypoint,truncated,finished_at
1,13,-0.0025248562070186156,0.01091450621758899,0.0,0.0,-0.07692307692307693,-0.019999999999999997,0.0,-0.032823130691242,0.14188858082865685,0.0,0.0,-1.0,-0.25999999999999995,0.0,0,False,True,1764576898.8251815
2,70,-0.0017850786910243077,0.04346829612536072,-1.2857142857142858,0.0,0.0,-0.02000000000000001,0.19265445237661963,-0.12495550837170154,3.0427807287752504,-90.0,0.0,0.0,-1.4000000000000008,13.485811666363373,9,True,False,1764576919.4029617
