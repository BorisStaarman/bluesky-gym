episode_index,steps,mean_reward_drift,mean_reward_progress,mean_reward_intrusion,mean_reward_path_efficiency,mean_reward_boundary,mean_reward_step,mean_reward_proximity,sum_reward_drift,sum_reward_progress,sum_reward_intrusion,sum_reward_path_efficiency,sum_reward_boundary,sum_reward_step,sum_reward_proximity,total_intrusions,terminated_waypoint,truncated,finished_at
1,22,-0.0002482469626103613,0.1084077780196321,-0.9090909090909091,0.0,0.0,-0.020000000000000004,0.1602808367183774,-0.005461433177427949,2.384971116431906,-20.0,0.0,0.0,-0.4400000000000001,3.5261784078043026,2,True,False,1764576900.0343502
2,48,-0.001985391899535674,0.014697152733459451,-1.6666666666666667,0.0,-0.020833333333333332,-0.02000000000000001,0.29364406966662254,-0.09529881117771234,0.7054633312060536,-80.0,0.0,-1.0,-0.9600000000000005,14.094915343997881,8,False,True,1764576916.8358412
