episode_index,steps,mean_reward_drift,mean_reward_progress,mean_reward_intrusion,mean_reward_path_efficiency,mean_reward_boundary,mean_reward_step,mean_reward_proximity,sum_reward_drift,sum_reward_progress,sum_reward_intrusion,sum_reward_path_efficiency,sum_reward_boundary,sum_reward_step,sum_reward_proximity,total_intrusions,terminated_waypoint,truncated,finished_at
1,80,-0.00249892174105045,0.012341829222132731,-1.0,0.0,-0.0125,-0.02000000000000001,0.28069986846745654,-0.199913739284036,0.9873463377706184,-80.0,0.0,-1.0,-1.600000000000001,22.45598947739652,8,False,True,1764576907.2050245
2,66,-0.002398311111010203,0.012984761627464146,-1.6666666666666667,0.0,-0.015151515151515152,-0.02000000000000001,0.15640561644279763,-0.1582885333266734,0.8569942674126336,-110.0,0.0,-1.0,-1.3200000000000007,10.322770685224643,11,False,True,1764576918.9495225
